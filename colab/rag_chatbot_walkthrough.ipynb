{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38dd576b",
      "metadata": {
        "id": "38dd576b"
      },
      "source": [
        "# **Project Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25cc3970",
      "metadata": {
        "id": "25cc3970"
      },
      "source": [
        "\n",
        "This project implements a Retrieval-Augmented Generation (RAG) system designed to support medical professionals in interpreting patient symptoms related to respiratory illnesses. The system retrieves relevant medical context from a curated dataset and generates medically grounded insights to aid clinical decision-making. The focus is on providing interpretable, high-precision, and ethically constrained AI support, not diagnostic conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9371c986",
      "metadata": {
        "id": "9371c986"
      },
      "source": [
        "## **Motivation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e02a908",
      "metadata": {
        "id": "9e02a908"
      },
      "source": [
        "Doctors often work with incomplete information and time constraints, especially in primary care or under-resourced settings. By surfacing concise, context-driven medical insights from trusted literature, this system aims to enhance a doctorâ€™s ability to make informed assessments, particularly in diagnosing and managing respiratory conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51f7f4af",
      "metadata": {
        "id": "51f7f4af"
      },
      "source": [
        "## **Ethical Considerations**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f264d49",
      "metadata": {
        "id": "6f264d49"
      },
      "source": [
        "Because the stakes in healthcare are high, this system avoids speculative output and includes a built-in disclaimer in every generated response. It explicitly avoids offering diagnoses, instead returning clinically relevant information strictly grounded in retrieved context. All data used is publicly available and de-identified, and outputs are designed to respect the professional judgment of medical practitioners rather than override it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e6b674",
      "metadata": {
        "id": "96e6b674"
      },
      "source": [
        "## **Dataset: PubMedQA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a42e54",
      "metadata": {
        "id": "50a42e54"
      },
      "source": [
        "The system uses the PubMedQA dataset, which contains over 211,000 question-context-answer triples derived from PubMed abstracts. To tailor this large dataset toward respiratory-specific medical use cases, a two-stage filtering process was applied:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6825dd",
      "metadata": {
        "id": "9f6825dd"
      },
      "source": [
        "### **1. Named Entity Recognition (NER)**\n",
        "\n",
        "We used the **en_ner_bc5cdr_md** model from **SciSpaCy**. This domain-specific model is trained on biomedical corpora to recognize both chemical and disease-related entities with high precision. Its targeted focus and strong performance in medical NER tasks make it well-suited for identifying passages related to respiratory conditions without relying solely on keyword matches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "938ec99e",
      "metadata": {
        "id": "938ec99e"
      },
      "source": [
        "### **2. Keyword Filtering**\n",
        "\n",
        "A comprehensive list of respiratory-related keywords was curated, covering diseases (e.g., pneumonia, COPD), anatomy (e.g., bronchi, alveoli), symptoms (e.g., wheezing, dyspnea), diagnostics (e.g., chest x-ray, spirometry), and treatments (e.g., corticosteroid, oxygen therapy). This ensured that the filtered subset captured a broad and clinically relevant representation of respiratory medicine."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa38e43",
      "metadata": {
        "id": "4aa38e43"
      },
      "source": [
        "## **Knowledge Base Construction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ac908a",
      "metadata": {
        "id": "63ac908a"
      },
      "source": [
        "After filtering, we extracted the contextual text associated with the retained QA pairs to form the knowledge base. This resulted in roughly 10,000 high-quality entries. To support scalability experiments and retrieval evaluation, we partitioned this filtered data into three knowledge base sizes:\n",
        "\n",
        "- **Small**: 2,000 entries\n",
        "\n",
        "- **Medium**: 6,000 entries\n",
        "\n",
        "- **Large**: 10,017 entries\n",
        "\n",
        "Each entry is embedded using **BioBERT** and stored in a transparent, inspectable structure (knowledgeBase object), enabling reproducible, interpretable retrieval. These knowledge bases form the foundation for evaluating how context size impacts retrieval accuracy and generation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fdedfe",
      "metadata": {
        "id": "22fdedfe"
      },
      "source": [
        "---\n",
        "\n",
        "# **1. Imports and Downloads**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Ensure consistent working directory\n",
        "os.chdir('/content')\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# Upload and process knowledge base and eval csv data\n",
        "print(\"\\nPlease upload the .zip containing our knowledge base and eval .csv data.\")\n",
        "uploaded = files.upload()\n",
        "zip_filename = next(iter(uploaded))\n",
        "csv_dirname = zip_filename.split(\".\")[0]\n",
        "!unzip \"{zip_filename}\"\n",
        "\n",
        "# Remove .zip file after extraction\n",
        "!rm \"{zip_filename}\"\n",
        "print(f\"\\n{zip_filename} uploaded and unzipped.\")"
      ],
      "metadata": {
        "id": "N2O3N-PzF7yu"
      },
      "id": "N2O3N-PzF7yu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9190c773",
      "metadata": {
        "id": "9190c773"
      },
      "source": [
        "## **1.2 Initial Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1cef49",
      "metadata": {
        "id": "ba1cef49",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q evaluate transformers accelerate google-generativeai rouge_score bert_score\n",
        "\n",
        "# Imports\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from pydantic import BaseModel\n",
        "import google.generativeai as genai\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import textwrap\n",
        "import pickle\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Take advantage of the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d624be98",
      "metadata": {
        "id": "d624be98"
      },
      "source": [
        "## **1.3 Load/Download Embedding Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aa30b5",
      "metadata": {
        "id": "71aa30b5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_name = \"dmis-lab/biobert-v1.1\"\n",
        "model_path = os.path.join(os.getcwd(), model_name)\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModel.from_pretrained(model_path)\n",
        "except Exception:\n",
        "    print(f\"Model not found at {model_path}. Downloading...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf7280d",
      "metadata": {
        "id": "fcf7280d"
      },
      "source": [
        "---\n",
        "\n",
        "# **2. Walkthrough: End-to-End RAG Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7704650c",
      "metadata": {
        "id": "7704650c"
      },
      "source": [
        "## **2.1 Definition: Knowledge Base Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de572ed",
      "metadata": {
        "id": "6de572ed"
      },
      "source": [
        "### **2.1.1 Description**\n",
        "\n",
        "This class enables retrieval of the most relevant medical texts based on cosine similarity between embeddings. It Stores entries as a dictionary keyed by unique IDs (PubMedQA row IDs). This class was designed to make the retrieval process fully transparent and interpretable, which was a key design goal for evaluation and explainability in a medical decision-support context.\n",
        "\n",
        "### **2.1.2 Functionality**\n",
        "- Add and retrieve entries\n",
        "- Calculate cosine similarity (assumes pre-normalized vectors)\n",
        "- Rank documents by relevance to a query embedding\n",
        "\n",
        "### **2.1.3 Key Methods**\n",
        "**__len__()**\n",
        "Allows easy inspection of the KB size using len(kb).\n",
        "\n",
        "**add_entry(...)**\n",
        "Adds a new text and its embedding into the KB.\n",
        "\n",
        "**search(...)**\n",
        "Performs brute-force cosine similarity search, returning the top-k most relevant texts for a given query embedding.\n",
        "\n",
        "**print_entries()**\n",
        "Prints the entire contents of the knowledge base. Helpful for debugging or inspecting entry structure.\n",
        "\n",
        "**get_entry_by_id(...)**\n",
        "Retrieve any entry by its unique ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24eddef3",
      "metadata": {
        "id": "24eddef3"
      },
      "outputs": [],
      "source": [
        "class knowledgeBase():\n",
        "    \"\"\"\n",
        "    A simple knowledge base for storing and retrieving text\n",
        "    entries based on their embeddings and cosine similarity.\n",
        "\n",
        "    Attributes:\n",
        "        knowledge (dict): A dictionary to store entries with\n",
        "        their unique IDs, text, embeddings, and cosine\n",
        "        similarity.\n",
        "    Methods:\n",
        "        __len__(): Returns the number of entries in the knowledge\n",
        "            base.\n",
        "        add_entry(entry_id, text, embedding): Adds an entry to\n",
        "            the knowledge base.\n",
        "        get_entry_by_id(entry_id): Retrieves an entry by its ID.\n",
        "        compute_cosine_similarity(vec_a, vec_b): Computes the\n",
        "            cosine similarity between two normalized vectors.\n",
        "        search_kb(q_embed, top_k): Searches the knowledge base\n",
        "            for the most relevant texts.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.knowledge = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of entries in the knowledge base.\n",
        "        \"\"\"\n",
        "        return len(self.knowledge)\n",
        "\n",
        "    def print_entries(self) -> None:\n",
        "        \"\"\"\n",
        "        Prints all entries in the knowledge base.\n",
        "        \"\"\"\n",
        "        for entry_id, entry_data in self.knowledge.items():\n",
        "            text_preview = entry_data['text'][:40]\n",
        "            embed_preview = entry_data['embedding'][:5]\n",
        "            cosine_similarity = entry_data['cosine_similarity']\n",
        "\n",
        "            print(f\"{entry_id}\\t{text_preview}...\\t{embed_preview}...\\t{cosine_similarity:.4f}\")\n",
        "\n",
        "    def add_entry(\n",
        "            self,\n",
        "            entry_id: int,\n",
        "            text: str,\n",
        "            embedding: np.ndarray\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Adds an entry to the knowledge base.\n",
        "\n",
        "        Args:\n",
        "            entry_id (int): Unique PubMed ID for the QA pair.\n",
        "            text (str): The text content of the entry.\n",
        "            embedding (np.ndarray): The embedding vector for the entry.\n",
        "        \"\"\"\n",
        "        # Text will be the context for the question-answer pair\n",
        "        self.knowledge[entry_id] = {\n",
        "            'pubid': entry_id,\n",
        "            'text': text,\n",
        "            'embedding': embedding,\n",
        "            'cosine_similarity': 0.0\n",
        "        }\n",
        "\n",
        "    def get_entry_by_id(\n",
        "            self,\n",
        "            entry_id: int\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Retrieves an entry from the knowledge base by its ID.\n",
        "\n",
        "        Args:\n",
        "            entry_id (int): The unique ID of the entry.\n",
        "        Returns:\n",
        "            dict: The entry data if found, otherwise None.\n",
        "        \"\"\"\n",
        "        return self.knowledge.get(entry_id, None)\n",
        "\n",
        "    def compute_cosine_similarity(\n",
        "            self,\n",
        "            vec_a: np.ndarray,\n",
        "            vec_b: np.ndarray\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Computes the cosine similarity between two normalized\n",
        "        vectors. This method assumes that the input vectors\n",
        "        have been pre-normalized through get_embedding().\n",
        "\n",
        "        Args:\n",
        "            vec_a (list or np.array): First vector.\n",
        "            vec_b (list or np.array): Second vector.\n",
        "        Returns:\n",
        "            float: Cosine similarity between the two vectors.\n",
        "        \"\"\"\n",
        "        return float(\n",
        "            np.dot(\n",
        "                vec_a.flatten(),\n",
        "                vec_b.flatten()\n",
        "        ))\n",
        "\n",
        "    def search(\n",
        "            self,\n",
        "            q_embed: np.ndarray,\n",
        "            top_k: int = 3\n",
        "    ) -> list[dict]:\n",
        "        \"\"\"\n",
        "        Search the knowledge base for the most relevant texts.\n",
        "\n",
        "        Args:\n",
        "            q_embed (np.ndarray): The user query embedding.\n",
        "            top_k (int): The number of top results to return.\n",
        "        Returns:\n",
        "            list[dict]: A list of dictionaries containing the\n",
        "                most relevant texts.\n",
        "        \"\"\"\n",
        "        # Compute cosine similarity for every kb entry\n",
        "        for entry_id, entry_data in self.knowledge.items():\n",
        "            embedding = entry_data['embedding']\n",
        "\n",
        "            # Calculate and populate cosine similarity\n",
        "            cosine_similarity = self.compute_cosine_similarity(q_embed, embedding)\n",
        "            self.knowledge[entry_id]['cosine_similarity'] = cosine_similarity\n",
        "\n",
        "        # Sort kb in descending order of cosine similarity\n",
        "        sorted_entries = sorted(\n",
        "            self.knowledge.values(),\n",
        "            key=lambda entry: entry['cosine_similarity'],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Return the top_k most relevant entries\n",
        "        return sorted_entries[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e76115",
      "metadata": {
        "id": "56e76115"
      },
      "source": [
        "## **2.2 Definition: RAG Pipeline Utilities**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765d63f1",
      "metadata": {
        "id": "765d63f1"
      },
      "source": [
        "### **2.2.1 Description**\n",
        "\n",
        "This cell defines key components for embedding generation, prompt construction, and knowledge base loading. It is instrumental to how the RAG system retrieves and formats relevant medical context for consistent generation.\n",
        "\n",
        "### **2.2.2 Key Components**\n",
        "\n",
        "**PROMPT_TEMPLATE**: A carefully designed prompt used to guide the language model's output:\n",
        "- Restricts responses to a single paragraph or short list\n",
        "- Ensures insights are grounded strictly in the retrieved context\n",
        "- Prevents speculative or diagnostic responses\n",
        "\n",
        "This format supports ethical use in a medical setting by promoting interpretability and limiting model overreach.\n",
        "\n",
        "### **2.2.3 Key Methods**\n",
        "\n",
        "**load_kb(file_path)**: Loads a serialized knowledgeBase object from disk using pickle. This allows re-use of pre-processed knowledge bases without re-generating embeddings every session.\n",
        "\n",
        "**get_context_prompt(user_query, results)**: Builds the final input string sent to the generative model by:\n",
        "- Concatenating the text fields of the top-k retrieved knowledge base entries\n",
        "- Injecting the user query into the structured prompt template\n",
        "\n",
        "This ensures consistency across all RAG inputs.\n",
        "\n",
        "**get_embedding(text)**: Generates a normalized embedding for the input text using the BioBERT model:\n",
        "- Tokenizes and sends input to the model (on GPU if available)\n",
        "- Normalizes the output vector to ensure cosine similarity is meaningful\n",
        "\n",
        "This function is central to both indexing and query-time retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd170dd",
      "metadata": {
        "id": "3cd170dd"
      },
      "outputs": [],
      "source": [
        "# Define consistent context prompt template for the RAG system\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Based only on the following medical context, provide a concise\n",
        "clinical insight to help a doctor interpret a patient's symptoms.\n",
        "The insight must be medically relevant, grounded in the provided\n",
        "context, and limited to a single paragraph or short list:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "Patient presents with the following symptoms:\n",
        "{prompt}\n",
        "\n",
        "Respond in a clear, medically appropriate tone. Do not speculate\n",
        "or provide a diagnosis.\n",
        "\"\"\"\n",
        "\n",
        "def load_kb(file_path: str) -> knowledgeBase:\n",
        "    \"\"\"\n",
        "    Load a knowledge base from a pickle file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the pickle file\n",
        "            containing the knowledge base.\n",
        "    Returns:\n",
        "        knowledgeBase: An instance of the knowledgeBase\n",
        "            class containing the loaded data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            kb = pickle.load(f)\n",
        "        return kb\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "        return None\n",
        "\n",
        "    except NotADirectoryError:\n",
        "        print(f\"Path error: {file_path} contains an invalid directory.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading knowledge base: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_context_prompt(\n",
        "        user_query: str,\n",
        "        results: list[dict]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create a formatted context prompt for the model.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query.\n",
        "        results (list[dict]): The top k results from the\n",
        "            knowledge base.\n",
        "    Returns:\n",
        "        str: The formatted context prompt.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join([\n",
        "        f\"{result['text']}\\n\"\n",
        "        for result in results\n",
        "    ])\n",
        "\n",
        "    return PROMPT_TEMPLATE.format(\n",
        "        context=context,\n",
        "        prompt=user_query\n",
        "    )\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Get the embedding of a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to get the embedding for.\n",
        "    Returns:\n",
        "        np.ndarray: The embedding for the text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Get the inputs\n",
        "    inputs = {\n",
        "        name: tensor.to(device)\n",
        "        for name, tensor in inputs.items()\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Normalize and return the embeddings\n",
        "    embed = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    embed = embed / np.linalg.norm(\n",
        "        embed,\n",
        "        axis=1,\n",
        "        keepdims=True\n",
        "    )\n",
        "\n",
        "    return embed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa059c00",
      "metadata": {
        "id": "fa059c00"
      },
      "source": [
        "## **2.3 Create the Knowledge Base**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd403fc0",
      "metadata": {
        "id": "bd403fc0"
      },
      "source": [
        "### **2.3.1 Description**\n",
        "\n",
        "This cell loads a serialized knowledge base from disk. Three sizes of knowledge bases have been prepared to facilitate testing and evaluation of the RAG system's performance across different scales:\n",
        "\n",
        "- Small: 2,000 entries\n",
        "- Medium: 6,000 entries\n",
        "- Large: ~10,000 entries\n",
        "\n",
        "By toggling the knowledge_base variable, system behavior across different knowledge base sizes can be tested. This flexibility supports evaluation of how knowledge base size affects retrieval and generation quality.\n",
        "\n",
        "The **load_kb()** function handles deserialization and returns a knowledgeBase object. Once loaded, the number of entries is printed for confirmation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb58999",
      "metadata": {
        "id": "8bb58999"
      },
      "outputs": [],
      "source": [
        "# Knowledge base sizes\n",
        "csv_name = 'Respiratory_Small_PubMedQA.csv'\n",
        "#csv_name = 'Respiratory_Medium_PubMedQA.csv'\n",
        "#csv_name = 'Respiratory_Large_PubMedQA.csv'\n",
        "\n",
        "# Load and process the input CSV file\n",
        "df = pd.read_csv(f\"/content/{csv_dirname}/{csv_name}\")\n",
        "texts = df['context'].tolist()\n",
        "batch_size = 64\n",
        "\n",
        "# Create a new knowledgeBase instance\n",
        "kb = knowledgeBase()\n",
        "\n",
        "total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "print(f\"Processing {len(texts)} texts in {total_batches} batches\")\n",
        "\n",
        "# Process texts in batches\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_indices = range(i, min(i + batch_size, len(texts)))\n",
        "    batch = [texts[j] for j in batch_indices]\n",
        "    batch_pubids = df.iloc[batch_indices]['pubid'].tolist()\n",
        "    batch_num = i // batch_size + 1\n",
        "    print(f\"Processing batch {batch_num}/{total_batches}...\", end=\"\\r\")\n",
        "\n",
        "    # Tokenize the batch\n",
        "    inputs = tokenizer(\n",
        "        batch,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move inputs to the appropriate device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the embeddings\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
        "    embeddings = embeddings.cpu().numpy()\n",
        "\n",
        "    # Add entries to the knowledge base using pubid as entry_id\n",
        "    for j, (text, embedding, pubid) in enumerate(zip(batch, embeddings, batch_pubids)):\n",
        "        kb.add_entry(pubid, text, embedding)\n",
        "\n",
        "    # Clear memory\n",
        "    del inputs, outputs\n",
        "    torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
        "\n",
        "print(f\"knowledgeBase of size {kb.__len__()} created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a6f3fb",
      "metadata": {
        "id": "f7a6f3fb"
      },
      "source": [
        "## **2.4 Define and Embed the Input Query**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97cd8da7",
      "metadata": {
        "id": "97cd8da7"
      },
      "source": [
        "### **2.4.1 Description**\n",
        "\n",
        "This cell defines a sample user query, simulating how a doctor might describe a patient's presenting symptoms.\n",
        "\n",
        "The **get_embedding()** function uses BioBERT to convert the input into a normalized embedding vector. This representation will be compared against all entries in the knowledge base using cosine similarity to identify the most semantically relevant medical contexts.\n",
        "\n",
        "A preview of the resulting embedding vector is printed to confirm successful encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1eaef7",
      "metadata": {
        "id": "8e1eaef7"
      },
      "outputs": [],
      "source": [
        "user_query = \"persistent cough, chest pain, and difficulty breathing\"\n",
        "\n",
        "query_embedding = get_embedding(user_query)\n",
        "print(f\"{query_embedding[0][:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bce7aae",
      "metadata": {
        "id": "0bce7aae"
      },
      "source": [
        "## **2.5 Search the Knowledge Base**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befd5277",
      "metadata": {
        "id": "befd5277"
      },
      "source": [
        "### **2.5.1 Description**\n",
        "\n",
        "This cell performs retrieval by comparing the query embedding to all entries in the knowledge base using cosine similarity. Each result includes a pubid (its unique identifier), a similarity score, and the matched context.\n",
        "\n",
        "The retrieved entries are printed in a structured table format, showing:\n",
        "- **pubid**: Unique identifier for the entry\n",
        "- **similarity**: Cosine similarity score indicating relevance to the query\n",
        "- **context**: The actual text content of the entry\n",
        "\n",
        "**Note**: Each entry includes its embedding representation, but it is not printed for brevity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3d9b8a",
      "metadata": {
        "id": "ea3d9b8a"
      },
      "outputs": [],
      "source": [
        "results = kb.search(query_embedding, top_k=3)\n",
        "sources = [result['pubid'] for result in results]\n",
        "\n",
        "print(\"ID\\t\\tSIMILARITY\\tCONTEXT\")\n",
        "\n",
        "for result in results:\n",
        "    print(f\"{result['pubid']}\\t{result['cosine_similarity']:.4f}\\t\\t{result['text'][:70]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2008f1",
      "metadata": {
        "id": "ee2008f1"
      },
      "source": [
        "## **2.6 Format the Context Prompt**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83073b3",
      "metadata": {
        "id": "e83073b3"
      },
      "source": [
        "### **2.6.1 Description**\n",
        "\n",
        "This cell prepares the final input prompt that will be sent to the generative model. It combines the userâ€™s input query with the retrieved context entries.\n",
        "\n",
        "The **get_context_prompt()** function fills these into a structured prompt template designed to guide the model's output.\n",
        "\n",
        "The printed result shows the complete, formatted prompt that ensures the modelâ€™s response remains grounded in retrieved medical content and adheres to the systemâ€™s clinical tone and ethical constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bab31fb",
      "metadata": {
        "id": "0bab31fb"
      },
      "outputs": [],
      "source": [
        "context_prompt = get_context_prompt(user_query, results)\n",
        "print(context_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a4e831",
      "metadata": {
        "id": "30a4e831"
      },
      "source": [
        "## **2.7 Query the Generative Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_model(query_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Query the model with a given query string.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The user's query.\n",
        "    Returns:\n",
        "        response (str): The model's response to the query.\n",
        "    \"\"\"\n",
        "    return gen_model.generate_content(context_prompt).text\n",
        "\n",
        "# Set the API key and generative model\n",
        "genai.configure(api_key=\"AIzaSyCG7d8xhcIWqJPgzs4a-BocMbN100YDR54\")\n",
        "gen_model = genai.GenerativeModel(\"gemma-3-27b-it\")\n",
        "\n",
        "# Send a prompt\n",
        "response = query_model(context_prompt)\n",
        "wrapped_response = textwrap.fill(response, width=70)\n",
        "print(wrapped_response + \"\\n\\nThis is not a diagnosis.\")"
      ],
      "metadata": {
        "id": "P09g5xUS7apa"
      },
      "id": "P09g5xUS7apa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c77f209a",
      "metadata": {
        "id": "c77f209a"
      },
      "source": [
        "### **2.7.1 Description**\n",
        "\n",
        "This cell sends the formatted context prompt to the generative model (LLaMa3.2-1b-Instruct) and generates an insight based on the retrieved medical context.\n",
        "\n",
        "**query_model(...)**: Returns a title and a detailed clinical insight relevant to the user's symptom query.\n",
        "\n",
        "A list of pubid values corresponding to the retrieved sources is included to provide traceability and transparency. This step demonstrates how the system synthesizes medically grounded information from multiple sources into a concise, context-aware response for decision support."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537b5eb5",
      "metadata": {
        "id": "537b5eb5"
      },
      "source": [
        "---\n",
        "\n",
        "# **3. System Evaluation: Retrieval and Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d343b39f",
      "metadata": {
        "id": "d343b39f"
      },
      "source": [
        "## **3.1 Retrieval: Metrics Defintion**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43877bc4",
      "metadata": {
        "id": "43877bc4"
      },
      "source": [
        "### **3.1.1 Description**\n",
        "\n",
        "This section defines core evaluation metrics for assessing the retrieval performance of the RAG system. Each metric is computed by comparing the similarity between retrieved chunks and the gold (ground truth) answer embeddings using a cosine similarity threshold.\n",
        "\n",
        "### **3.1.2 Metrics Implemented**\n",
        "- **Recall@k**: Measures whether at least one relevant chunk appears in the top-k results.\n",
        "- **Precision@k**: Measures how many of the top-k chunks are actually relevant.\n",
        "- **Mean Reciprocal Rank (MRR)**: Evaluates how early the first relevant chunk appears in the top-k list.\n",
        "\n",
        "### **3.1.3 Function Descriptions**\n",
        "**get_cached_embedding(text)**\n",
        "Caches embedding results to avoid redundant model calls, improving performance during evaluation.\n",
        "\n",
        "**compute_bin_recall(retrieved_chunks, gold_embedding, thresh)**\n",
        "Returns 1 if any retrieved chunk exceeds the similarity threshold with the gold embedding, otherwise 0.\n",
        "\n",
        "**compute_precision(retrieved_chunks, gold_embedding, thresh)**\n",
        "Returns the fraction of retrieved chunks that exceed the similarity threshold, representing precision.\n",
        "\n",
        "**compute_mrr(retrieved_chunks, gold_embedding, thresh)**\n",
        "Returns the reciprocal rank of the first relevant chunk. If none are above the threshold, returns 0.0.\n",
        "\n",
        "**retrieval_eval_at_k(df, thresh, k)**\n",
        "Applies the above metrics over all rows in a list of 30 sample QA pairs loaded from **eval.csv**. For each question-answer pair:\n",
        "- Embeds the query and gold answer\n",
        "- Retrieves top-k chunks from the knowledge base\n",
        "- Computes Recall@k, Precision@k, and MRR\n",
        "\n",
        "The function returns three lists of scores, one for each metric, suitable for averaging and further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f287de2",
      "metadata": {
        "id": "0f287de2"
      },
      "outputs": [],
      "source": [
        "embedding_cache = {}\n",
        "\n",
        "def get_cached_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieves the embedding for a given text, using a cache\n",
        "    to avoid redundant computations.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to get the embedding for.\n",
        "    Returns:\n",
        "        np.ndarray: The embedding vector for the input text.\n",
        "    \"\"\"\n",
        "    if text in embedding_cache:\n",
        "        return embedding_cache[text]\n",
        "    else:\n",
        "        embedding = get_embedding(text)\n",
        "        embedding_cache[text] = embedding\n",
        "        return embedding\n",
        "\n",
        "def compute_bin_recall(\n",
        "        retrieved_chunks: list[dict],\n",
        "        gold_embedding: np.ndarray,\n",
        "        thresh: float = 0.7\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Computes binary recall for k retrieved chunks against\n",
        "    gold answer. (Tests if there is at least one relevant\n",
        "    chunk in the top-k results.)\n",
        "\n",
        "    Args:\n",
        "        retrieved_chunks (list): The retrieved chunks.\n",
        "        gold_embedding (np.ndarray): The gold answer embedding.\n",
        "    Returns:\n",
        "        int: 1 if gold answer is found in any chunk, else 0.\n",
        "    \"\"\"\n",
        "    # Check if any chunk exceeds the similarity threshold\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_embedding = get_cached_embedding(chunk['text'])\n",
        "        similarity = kb.compute_cosine_similarity(\n",
        "            gold_embedding,\n",
        "            chunk_embedding\n",
        "        )\n",
        "\n",
        "        if similarity >= thresh:\n",
        "            return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def compute_precision(\n",
        "        retrieved_chunks: list[dict],\n",
        "        gold_embedding: np.ndarray,\n",
        "        thresh: float = 0.7\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes precision for k retrieved chunks against gold answer.\n",
        "\n",
        "    Args:\n",
        "        retrieved_chunks (list): The retrieved chunks.\n",
        "        gold_embedding (np.ndarray): The gold answer embedding.\n",
        "    Returns:\n",
        "        float: The number of matches over total number of chunks.\n",
        "    \"\"\"\n",
        "    total_chunks = len(retrieved_chunks)\n",
        "    matches = 0\n",
        "\n",
        "    if total_chunks == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Count how many chunks exceed the similarity threshold\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_embedding = get_cached_embedding(chunk['text'])\n",
        "        similarity = kb.compute_cosine_similarity(\n",
        "            gold_embedding,\n",
        "            chunk_embedding\n",
        "        )\n",
        "\n",
        "        if similarity >= thresh:\n",
        "            matches += 1\n",
        "\n",
        "    return matches / total_chunks\n",
        "\n",
        "def compute_mrr(\n",
        "        retrieved_chunks: list[dict],\n",
        "        gold_embedding: np.ndarray,\n",
        "        thresh: float = 0.7\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes Mean Reciprocal Rank (MRR) for k retrieved chunks\n",
        "    against gold answer.\n",
        "\n",
        "    Args:\n",
        "        retrieved_chunks (list): The retrieved chunks.\n",
        "        gold_embedding (np.ndarray): The gold answer embedding.\n",
        "    Returns:\n",
        "        float: The reciprocal of the rank of the first match,\n",
        "            or 0 if no match is found.\n",
        "    \"\"\"\n",
        "    # Find the rank of the first chunk that exceeds the threshold\n",
        "    for rank, chunk in enumerate(retrieved_chunks, start=1):\n",
        "        chunk_embedding = get_cached_embedding(chunk['text'])\n",
        "        similarity = kb.compute_cosine_similarity(\n",
        "            gold_embedding,\n",
        "            chunk_embedding\n",
        "        )\n",
        "\n",
        "        if similarity >= thresh:\n",
        "            return 1.0 / rank\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "def retrieval_eval_at_k(\n",
        "        df: pd.DataFrame,\n",
        "        thresh: float = 0.7,\n",
        "        k: int = 3\n",
        ") -> tuple[list]:\n",
        "    \"\"\"\n",
        "    Evaluates the retrieval performance of a RAG system using\n",
        "    Recall, Precision, and Mean Reciprocal Rank (MRR) metrics.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing\n",
        "            'context' and 'long_answer' columns.\n",
        "        thresh (float): The threshold for cosine similarity to\n",
        "            consider a match.\n",
        "        k (int): The number of top results to consider\n",
        "            for evaluation.\n",
        "    Returns:\n",
        "        tuple: A tuple containing three lists:\n",
        "            - Recall@k scores\n",
        "            - Precision@k scores\n",
        "            - MRR scores\n",
        "    \"\"\"\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "    mrr_scores = []\n",
        "\n",
        "    # Calculate scores for each row in the eval DataFrame\n",
        "    for question, long_answer in zip(df['question'], df['long_answer']):\n",
        "        prompt_embedding = get_embedding(question)\n",
        "        results = kb.search(q_embed=prompt_embedding, top_k=k)\n",
        "        gold_embedding = get_embedding(long_answer)\n",
        "\n",
        "        recall_scores.append(compute_bin_recall(\n",
        "            results,\n",
        "            gold_embedding,\n",
        "            thresh\n",
        "        ))\n",
        "        precision_scores.append(compute_precision(\n",
        "            results,\n",
        "            gold_embedding,\n",
        "            thresh\n",
        "        ))\n",
        "        mrr_scores.append(compute_mrr(\n",
        "            results,\n",
        "            gold_embedding,\n",
        "            thresh\n",
        "        ))\n",
        "\n",
        "    return recall_scores, precision_scores, mrr_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ae0467",
      "metadata": {
        "id": "d3ae0467"
      },
      "source": [
        "## **3.2 Retrieval: Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04d8b64",
      "metadata": {
        "id": "c04d8b64"
      },
      "source": [
        "### **3.2.1 Description**\n",
        "\n",
        "This cell evaluates the retrieval performance of the knowledge base against a sample of 30 QA pairs loaded from **eval.csv**.\n",
        "\n",
        "- **thresh = 0.75**: A cosine similarity of 0.75 or higher is required to count as a relevant match.\n",
        "- **top_k = 3**: Only the top 3 retrieved chunks are evaluated per query.\n",
        "\n",
        "The results are averaged across all queries and printed for inspection. These scores help validate the systemâ€™s ability to retrieve clinically relevant context for generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc787052",
      "metadata": {
        "id": "fc787052"
      },
      "outputs": [],
      "source": [
        "# Load the evaluation data from a CSV file\n",
        "csv_path = f'/content/{csv_dirname}/eval.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Experiment with these parameters to optimize retrieval performance:\n",
        "thresh = 0.75   # Cosine similarity threshold for relevance\n",
        "top_k = 3       # Top-k chunks to retrieve for evaluation\n",
        "\n",
        "# Calculate retrieval evaluation metrics\n",
        "recall, precision, mrr = retrieval_eval_at_k(\n",
        "    df=df,\n",
        "    thresh=thresh,\n",
        "    k=top_k\n",
        ")\n",
        "\n",
        "# Calculate average scores\n",
        "recall_avg = sum(recall) / len(recall)\n",
        "precision_avg = sum(precision) / len(precision)\n",
        "mrr_avg = sum(mrr) / len(mrr)\n",
        "\n",
        "print(f\"Retrieval Evaluation Results for {len(df)} queries:\")\n",
        "print(f\"Recall@{top_k}:\\t{recall_avg:.4f}\")\n",
        "print(f\"Precision@{top_k}:\\t{precision_avg:.4f}\")\n",
        "print(f\"MRR:\\t\\t{mrr_avg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "866eaddd",
      "metadata": {
        "id": "866eaddd"
      },
      "source": [
        "### **3.2.1 Results Analysis**\n",
        "\n",
        "These results indicate that the retrieval system performed exceptionally well across the 30 evaluation queries:\n",
        "\n",
        "- **Recall@3 = 1.0000**: Confirms that for every query, at least one relevant chunk was retrieved in the top 3 results.\n",
        "- **Precision@3 = 0.9667**: Shows that nearly all of the top 3 retrieved chunks were relevant, indicating very low noise in the returned context.\n",
        "- **MRR = 1.0000**: Means the first relevant chunk was always ranked first, demonstrating highly effective relevance ranking.\n",
        "\n",
        "Together, these metrics demonstrate that the retrieval component consistently delivers high-quality, contextually appropriate results, which are critical for supporting the downstream generation step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f956477f",
      "metadata": {
        "id": "f956477f"
      },
      "source": [
        "## **3.3 Retrieval: Plotting Performance for Similarity Threshold Optimization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f54fd9",
      "metadata": {
        "id": "07f54fd9"
      },
      "source": [
        "### **3.3.1 Description**\n",
        "\n",
        "This section evaluates how retrieval performance varies across different cosine similarity thresholds. The goal is to identify an optimal threshold that balances recall, precision, and ranking quality (MRR).\n",
        "\n",
        "**calculate_metrics(df)**\n",
        "- Iterates through cosine similarity thresholds from 0.70 to 0.85 in steps of 0.01.\n",
        "- For each threshold, each retrieval metric is computed as defined in section **3.1 Retrieval: Metrics Definition**.\n",
        "- Returns lists of thresholds and the corresponding average scores.\n",
        "\n",
        "**plot_results(...)**\n",
        "- Plots each metric (Recall@3, Precision@3, MRR) against the evaluated thresholds.\n",
        "\n",
        "The resulting graph is used to visually identify the threshold that yields the best overall retrieval performance. This analysis supports the selection of a well-justified, optimal similarity cutoff (e.g., 0.75). This can be explored further by switching out the knowledge base size to see how retrieval performance varies with data scale.\n",
        "\n",
        "### **3.3.2 Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4ceae7",
      "metadata": {
        "id": "3a4ceae7"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(df: pd.DataFrame) -> tuple:\n",
        "    \"\"\"\n",
        "    Calculates retrieval evaluation metrics (Recall, Precision,\n",
        "    MRR) for varying cosine similarity thresholds.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing evaluation data.\n",
        "    Returns:\n",
        "        tuple: A tuple containing lists of thresholds, recall\n",
        "            scores, precision scores, and MRR scores.\n",
        "    \"\"\"\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "    mrr_scores = []\n",
        "    thresholds = []\n",
        "    thresh = 0.7\n",
        "\n",
        "    # Iterate over thresholds from 0.7 to 0.85\n",
        "    while thresh <= 0.85:\n",
        "        # Round the threshold and calculate metrics\n",
        "        thresholds.append(round(thresh, 2))\n",
        "        recall, precision, mrr = retrieval_eval_at_k(\n",
        "            df=df,\n",
        "            thresh=thresh,\n",
        "            k=3\n",
        "        )\n",
        "\n",
        "        # Calculate and append average scores\n",
        "        recall_avg = sum(recall) / len(recall)\n",
        "        precision_avg = sum(precision) / len(precision)\n",
        "        mrr_avg = sum(mrr) / len(mrr)\n",
        "        recall_scores.append(recall_avg)\n",
        "        precision_scores.append(precision_avg)\n",
        "        mrr_scores.append(mrr_avg)\n",
        "\n",
        "        thresh += 0.01\n",
        "\n",
        "    return (\n",
        "        thresholds,\n",
        "        recall_scores,\n",
        "        precision_scores,\n",
        "        mrr_scores\n",
        "    )\n",
        "\n",
        "def plot_results(\n",
        "        thresholds: list[float],\n",
        "        recall_scores: list[float],\n",
        "        precision_scores: list[float],\n",
        "        mrr_scores: list[float]\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots the retrieval evaluation metrics against cosine\n",
        "    similarity thresholds and saves the plot to a file.\n",
        "\n",
        "    Args:\n",
        "        thresholds (list[float]): List of cosine similarity\n",
        "            thresholds.\n",
        "        recall_scores (list[float]): List of Recall@3 scores.\n",
        "        precision_scores (list[float]): List of Precision@3 scores.\n",
        "        mrr_scores (list[float]): List of MRR scores.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(\n",
        "        thresholds,\n",
        "        recall_scores,\n",
        "        label='Recall@3',\n",
        "        marker='o'\n",
        "    )\n",
        "    plt.plot(\n",
        "        thresholds,\n",
        "        precision_scores,\n",
        "        label='Precision@3',\n",
        "        marker='o'\n",
        "    )\n",
        "    plt.plot(\n",
        "        thresholds,\n",
        "        mrr_scores,\n",
        "        label='MRR',\n",
        "        marker='o'\n",
        "    )\n",
        "\n",
        "    # Set plot title and labels\n",
        "    plt.title('Retrieval Eval Scores vs Cosine Similarity Threshold')\n",
        "    plt.text(\n",
        "        0.7, 0.9,\n",
        "        f\"Knowledge Base Size: {kb.__len__()} entries\",\n",
        "        transform=plt.gca().transAxes,\n",
        "        fontsize=10,\n",
        "        verticalalignment='top'\n",
        "    )\n",
        "    plt.xlabel('Cosine Similarity Threshold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.xticks(thresholds)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1a8f25",
      "metadata": {
        "id": "1e1a8f25"
      },
      "source": [
        "### **3.3.3 Generating Plots**\n",
        "\n",
        "This cell loads the evaluation dataset and generates the plot for each knowledge base, helping visualize the retrieval performance across thresholds and kb sizes.\n",
        "\n",
        "This visualization helps determine the sweet spot for the cosine similarity threshold, where the system maintains high recall while maximizing precision and ranking quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e75fe43",
      "metadata": {
        "id": "7e75fe43"
      },
      "outputs": [],
      "source": [
        "# Load the evaluation data from a CSV file\n",
        "csv_path = f'/content/{csv_dirname}/eval.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Calculate metrics for varying thresholds\n",
        "results = calculate_metrics(df=df)\n",
        "\n",
        "# Extract results\n",
        "thresholds = results[0]\n",
        "recall_scores = results[1]\n",
        "precision_scores = results[2]\n",
        "mrr_scores = results[3]\n",
        "\n",
        "# Plot and save the results\n",
        "plot_results(\n",
        "    thresholds=thresholds,\n",
        "    recall_scores=recall_scores,\n",
        "    precision_scores=precision_scores,\n",
        "    mrr_scores=mrr_scores\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90996d9",
      "metadata": {
        "id": "a90996d9"
      },
      "source": [
        "### **3.3.4 Results Analysis**\n",
        "\n",
        "Across all three knowledge base sizes, the plots reveal a consistent trend: as the cosine similarity threshold increases beyond 0.75, both Precision@3 and MRR begin to decline, while Recall@3 remains flat at 1.0 up to a point and then begins to drop.\n",
        "\n",
        "This trend reflects a common retrieval trade-off: higher thresholds filter out more marginal matches, improving relevance (up to a point), but risk discarding useful context if set too high. At thresholds above 0.75, the stricter filter begins to exclude relevant chunks, causing precision and rank quality to deteriorate.\n",
        "\n",
        "Based on these patterns, 0.75 emerges as the optimal threshold for use as an additional similarity filter. It maximizes precision and ranking quality without sacrificing recall, ensuring that the context passed to the language model remains both relevant and high quality. This is especially important in a clinical setting where irrelevant or noisy input could reduce the reliability of generated insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcbaafd",
      "metadata": {
        "id": "abcbaafd"
      },
      "source": [
        "## **3.4 Generation: Metrics Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621bd519",
      "metadata": {
        "id": "621bd519"
      },
      "source": [
        "### **3.4.1 Description**\n",
        "\n",
        "This section defines how the system evaluates the quality of its generated outputs against reference answers using three standard metrics from the **Hugging Face evaluate** library.\n",
        "\n",
        "### **3.4.2 Metrics Implemented**\n",
        "- **BERTScore**: Computes semantic similarity using BERT embeddings, capturing meaning beyond surface word matches. Higher scores indicate stronger semantic alignment. For the purposes of this project, BERTScore is treated as the primary generation evaluation metric, as it captures semantic similarity between the generated and reference responses, rather than relying solely on lexical overlap. ROUGE-L and BLEU are included to provide complementary perspectives on surface-level similarity and structural alignment.\n",
        "- **ROUGE-L**: Measures the longest common subsequence between the generated text and reference text. It captures the fluency and coverage of the generated text compared to the reference. Higher ROUGE-L scores indicate better content coverage.\n",
        "- **BLEU**: Measures n-gram precision between the generated and reference texts. Higher scores reflect greater lexical overlap and phrasing accuracy.\n",
        "\n",
        "### **3.4.3 Function Breakdown**\n",
        "**compute_rouge_l(prediction, reference)**:\n",
        "Returns the ROUGE-L score between a generated response and the reference.\n",
        "\n",
        "**compute_bleu(prediction, reference)**:\n",
        "Returns the BLEU score, using reference n-gram matching.\n",
        "\n",
        "**compute_bertscore(prediction, reference)**:\n",
        "Returns the BERTScore F1 value, capturing semantic overlap.\n",
        "\n",
        "**generation_eval(df)**:\n",
        "Runs the full evaluation pipeline over the **eval.csv** dataset (30 question-answer pairs). For each:\n",
        "- It embeds the question\n",
        "- Retrieves the top 3 most relevant context chunks\n",
        "- Generates a response from the RAG system\n",
        "- Compares the output to the gold answer using all three metrics\n",
        "\n",
        "The result is a tuple of lists containing the per-query scores, which are later averaged to produce final evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16534446",
      "metadata": {
        "id": "16534446"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "def compute_rouge_l(\n",
        "        prediction: str,\n",
        "        reference: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the ROUGE score for the generated text against\n",
        "    the reference text.\n",
        "\n",
        "    Args:\n",
        "        prediction (str): The generated text.\n",
        "        reference (str): The reference text.\n",
        "    Returns:\n",
        "        dict: The ROUGE score.\n",
        "    \"\"\"\n",
        "    return rouge.compute(\n",
        "        predictions=[prediction],\n",
        "        references=[reference]\n",
        "    )['rougeL']\n",
        "\n",
        "def compute_bleu(\n",
        "        prediction: str,\n",
        "        reference: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the BLEU score for the generated text against\n",
        "    the reference text.\n",
        "\n",
        "    Args:\n",
        "        prediction (str): The generated text.\n",
        "        reference (str): The reference text.\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "    return bleu.compute(\n",
        "        predictions=[prediction],\n",
        "        references=[[reference]]\n",
        "    )['bleu']\n",
        "\n",
        "def compute_bertscore(\n",
        "        prediction: str,\n",
        "        reference: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the BERTScore for the generated text against\n",
        "    the reference text.\n",
        "\n",
        "    Args:\n",
        "        prediction (str): The generated text.\n",
        "        reference (str): The reference text.\n",
        "    Returns:\n",
        "        dict: The F1 score of the BERTScore.\n",
        "    \"\"\"\n",
        "    return bertscore.compute(\n",
        "        predictions=[prediction],\n",
        "        references=[reference],\n",
        "        lang='en'\n",
        "    )['f1'][0]\n",
        "\n",
        "def generation_eval(df: pd.DataFrame) -> tuple[list]:\n",
        "    \"\"\"\n",
        "    Evaluates the generation model using ROUGE, BLEU, and\n",
        "    BERTScore metrics.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing\n",
        "            'context' and 'long_answer' columns.\n",
        "    Returns:\n",
        "        tuple: A tuple containing three lists:\n",
        "            - ROUGE scores\n",
        "            - BLEU scores\n",
        "            - BERTScores\n",
        "    \"\"\"\n",
        "    rougel_scores = []\n",
        "    bleu_scores = []\n",
        "    bertscore_scores = []\n",
        "\n",
        "    # Calculate scores for each row in the eval DataFrame\n",
        "    for question, long_answer in zip(df['question'], df['long_answer']):\n",
        "        prompt_embedding = get_embedding(question)\n",
        "        results = kb.search(q_embed=prompt_embedding, top_k=3)\n",
        "        context_prompt = get_context_prompt(question, results)\n",
        "        time.sleep(2)\n",
        "        generated = query_model(context_prompt)\n",
        "\n",
        "        bleu_scores.append(compute_bleu(\n",
        "            generated,\n",
        "            long_answer\n",
        "        ))\n",
        "        rougel_scores.append(compute_rouge_l(\n",
        "            generated,\n",
        "            long_answer\n",
        "        ))\n",
        "        bertscore_scores.append(compute_bertscore(\n",
        "            generated,\n",
        "            long_answer\n",
        "        ))\n",
        "\n",
        "    return rougel_scores, bleu_scores, bertscore_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3752faff",
      "metadata": {
        "id": "3752faff"
      },
      "source": [
        "## **3.5 Generation: Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a64f511d",
      "metadata": {
        "id": "a64f511d"
      },
      "source": [
        "### **3.5.1 Description**\n",
        "\n",
        "This cell evaluates the quality of the RAG system's generated responses across 30 question-answer pairs from **eval.csv**. The **generation_eval()** function computes per-query scores using BERTScore, ROUGE-L, and BLEU. The scores are then averaged to produce final evaluation metrics.\n",
        "\n",
        "The printed output summarizes overall generation performance across the evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394aea9f",
      "metadata": {
        "id": "394aea9f"
      },
      "outputs": [],
      "source": [
        "# Load the evaluation data from a CSV file\n",
        "csv_path = f'/content/{csv_dirname}/eval.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "rougel, bleu, bertscore = generation_eval(df=df)\n",
        "\n",
        "# Calculate the average scores\n",
        "rougel_avg = sum(rougel) / len(df)\n",
        "bleu_avg = sum(bleu) / len(df)\n",
        "bert_score_avg = sum(bertscore) / len(df)\n",
        "\n",
        "print(f\"\\nGeneration Evaluation Results for {len(df)} queries:\")\n",
        "print(f\"BERTScore:\\t{bert_score_avg:.4f}\")\n",
        "print(f\"ROUGE-L:\\t{rougel_avg:.4f}\")\n",
        "print(f\"BLEU:\\t\\t{bleu_avg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb49e179",
      "metadata": {
        "id": "bb49e179"
      },
      "source": [
        "### **3.5.2 Results Analysis**\n",
        "\n",
        "These scores reflect the systemâ€™s ability to generate relevant and well-grounded responses:\n",
        "\n",
        "- **BERTScore = 0.8605**: Indicates strong semantic similarity between generated and reference answers, suggesting that the model captures the intended meaning even when phrasing differs.\n",
        "- **ROUGE-L = 0.1492**: Shows moderate overlap in content structure, reflecting partial alignment in how information is expressed.\n",
        "- **BLEU = 0.0160**: Is low, which is expected in this setting, as the model does not aim to reproduce exact phrasing but rather convey medically accurate insights.\n",
        "\n",
        "Overall, the generation component performs well semantically, prioritizing meaning over surface similarity, which is consistent with the systemâ€™s goal of producing clinically relevant, context-aware outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **4. Clean Up**"
      ],
      "metadata": {
        "id": "NpEeI-Y2PLkX"
      },
      "id": "NpEeI-Y2PLkX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 Delete Uploaded .csv Data**"
      ],
      "metadata": {
        "id": "HaPmEw-4RCDw"
      },
      "id": "HaPmEw-4RCDw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block deletes all uploaded .csv files used for the knowledge base and evaluation, then removes the directory itself. Run this to clean up your Drive and remove the temporary data dependencies created at the start of the notebook."
      ],
      "metadata": {
        "id": "wOMblKF7QCyp"
      },
      "id": "wOMblKF7QCyp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the directory containing the knoweldge base and eval .csv data\n",
        "!rm -rf \"{csv_dirname}\""
      ],
      "metadata": {
        "id": "R4YNASogPN6N"
      },
      "id": "R4YNASogPN6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Delete Cached BioBERT Embedding Model**"
      ],
      "metadata": {
        "id": "oj0_4PPCRIms"
      },
      "id": "oj0_4PPCRIms"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block deletes the directory containing the cached BioBERT embedding model. Run this to free up space and remove the model files downloaded during this notebook's execution."
      ],
      "metadata": {
        "id": "qMELOwbzSV_0"
      },
      "id": "qMELOwbzSV_0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the directory containing the cached BioBERT embedding model\n",
        "embedding_model_cache_path = os.path.join(os.getcwd(), os.path.dirname(model_name))\n",
        "!rm -rf \"{embedding_model_cache_path}\""
      ],
      "metadata": {
        "id": "dhKGATRYRMmx"
      },
      "id": "dhKGATRYRMmx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "38dd576b",
        "9371c986",
        "51f7f4af",
        "96e6b674",
        "4aa38e43",
        "537b5eb5",
        "d343b39f",
        "d3ae0467",
        "f956477f",
        "abcbaafd",
        "3752faff"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}